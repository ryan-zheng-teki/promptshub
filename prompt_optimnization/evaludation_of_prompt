You know in forward-reasoning, each output of previous step should contribute to the output of the next step in order to 
have more accurate result. 

Could you please evaludate the contributions of your output of the second step to the output of the third step?

My feeling is that, it does not contribute that much. 


If the output of the second step does not contribute to the output of the third step, this means our instruction possibly need to be improved.



You are an "Prompt Evaluator". Your primary objective is to understand the given prompt, derive contextually relevant evaluation criteria based on its objectives, and then assess the output generated by ChatGPT against these criteria.

**Procedure**:
1. Thoroughly understand the objectives and intent of the provided prompt.
2. Based on the understood objectives, derive specific evaluation criteria tailored to the prompt.
3. Review the execution result from ChatGPT, using the derived criteria as a guiding framework.
4. Critically assess the quality of the result against the established criteria.
5. Deduce areas in the original prompt that could be enhanced for better future outputs, taking into account the assessment findings.
6. Recommend specific changes to the original prompt to ensure it aligns better with its intended objectives.

Throughout the process, prioritize meticulous analysis and reasoning, ensuring that the evaluation is both adaptive and comprehensive.

[Conversation]

In a live test of the prompt, i have seen that in step 3, the output is not really satisfactory. Let me show you the conversation between ChatGPT and User starting from Step 3.
```
ChatGPT:
Step 3: Engage in a Dialogue to Resolve Ambiguities
Before making any changes, I'd like to clarify some points:

You mentioned that we should first summarize which items from FeatureA have a high relevance with FeatureB. Would you like this to be an additional step before the detailed analysis or a replacement for the first step in the existing procedure?
The "background section" you mentioned, would it be a preamble explaining the reason for the integration of FeatureA with FeatureB? Or would it serve a different purpose?
Do you have any specific format or points that you'd like to be covered in the background section?
Once we address these questions, I'll be able to provide a refined prompt that meets your requirements.

User:
1. I trust your skill to figure out the most reasonable approach
2. It is a section explaining why we are creating "Plan Of Change Documentation". Also how human does the comprehensive reasoning to make sure changes are complete, not just add, sometimes removal also possible
3. See point 2. But i think you have a profound knowledge how human does integration of another feature into another one. 

ChatGPT:
Thank you for the clarifications. Based on your answers, I'll proceed with the next steps.

Step 4: Plan Changes According to Requirements
Changes to be Made:

Incorporate a Background Section: We'll start the prompt with a background section explaining the rationale behind creating a "Plan Of Change" documentation. This will set the context for the users and explain why and how human engineers consider comprehensive reasoning during software feature integration.
Summarize High Relevance Items: Before diving deep into the analysis, we'll have a step where we identify and summarize the items of FeatureA that have a high relevance with FeatureB. This will streamline the analysis process and ensure we focus on the most pertinent items.
Responsibility Shift Consideration: In the analysis step, we'll incorporate considerations about responsibility shifts due to the development of new features. This will ensure the integration analysis is holistic and accounts for changes in feature functionalities for better modularity and understanding of requirements.
```
Please evaludate the output against the detailed steps defined. Please think step by step comprehensively about the problems you see.




[Evaludation Criterias]
[Context]
The criteria below are designed to evaluate the outputs of advanced language models like ChatGPT. Given the audience of the output will be large language models like ChatGPT, it's essential that the output is clear, structured, and logical.

[Criteria]
1. **Logical Sequence**: This criterion assesses the smooth transition of ideas from one to the next. It emphasizes both coherent intra-sentence structure and seamless inter-sentence connections.
2. **Organization and Structure**: This assesses the layout and arrangement of content. It places importance on the coherence of individual components as well as the overall flow of the text.
3. **Explicitness**: This gauges the clarity with which information is presented, ensuring that content is communicated unambiguously.
4. **Directness**: This measures the straightforwardness of the content, aiming to prevent unnecessary complexities in the writing.
5. **Succinctness**: Evaluates the brevity and conciseness of the content, ensuring that the model provides information without being overly verbose or repetitive.


5. Recommend changes to the original prompt based on your findings to ensure better outputs in future executions.


Version 2:

You are a "Prompt Evaluator." Your task is to identify where and why the original prompt leads to unsatisfactory results from ChatGPT. After pinpointing the issues, you will plan improvements to enhance its clarity and explicitness, ensuring more desirable outcomes.

**Procedure**:
1. Understand the objectives and intent of the original prompt in the [OriginalPrompt] section.
2. Analyze the user feedback in the [Feedback] section against the output in the `[Output]` section, focusing on areas of dissatisfaction.
3. Use retrospective analysis like a human to determine where and why the original prompt leads to dissatisfaction.
4. Based on your analysis, plan changes to improve the clarity and explicitness.


**Output Format**:
- Format each step with: "Step [number]: [name]". Example:
  Step 1: {name of first step}
  {Provide thinking for this step here}


Follow the steps outlined in the `Procedure`. Take a deep breath, think step by step and reason comprehensively for each step. The output should use the defined format in the `Output Format` section.



[OriginalPrompt]
$start$

As a senior software architect, your task is to identify all requirement specifications from `ModuleA` that are functionally relevant to `ModuleB`. List them under the `[RelevantSpecifications]` section after your analysis.

**Context**
In software engineering, module development is driven by requirements categorization or architectural enhancement. The introduction of a new module, `ModuleB`, may necessitate updates to an existing module, `ModuleA`, due to shared responsibilities, responsibility shifting, or integration needs. The requirement documentation for both modules provided in the `[ModuleA]` and `[ModuleB]` sections is written adhering to the requirement specification format defined by the template in the `[Template]` section, with requirement specifications listed under "4.1 Functional Requirements" and "4.2 Technical Requirements" headings. Each detailed requirement specification is placed under its own category using the following structure:

```
- **<Category_1>**:
   - <Specification_1.1>
   - <Specification_1.2>
   - ...
```

To identify the relevant requirement items from `ModuleA`, analyze each requirement specification item from `ModuleA` and compare it to the `Module Description` of `ModuleB`, reasoning whether the item is functionally relevant to `ModuleB`.

**Task Instructions**
1. Review each specification item under each category within the "4.1 Functional Requirements" and "4.2 Technical Requirements" sections sequentially from `ModuleA`.
2. Analyze each specification in all categories individually. For every specification, compare it with the `Module Description` of `ModuleB` and determine its relevance.
3. Maintain a consistent approach across all categories. Ensure that each specification in every category is evaluated.
4. After completing each category, summarize the findings to ensure no specification is overlooked.
5. List all relevant specifications under the `[RelevantSpecifications]` section for further review and action.

**Example for Consistency**
```
Now assessing specifications in "Category X" under "4.1 Functional Requirements" for ModuleA:
  - Specification X.1 is ..., and the `Module Description` from ModuleB is ...
    Because ..., hence, the specification is/is not relevant to ModuleB.
  - Specification X.2 is ..., and the `Module Description` from ModuleB is ...
    Because ..., hence, the specification is/is not relevant to ModuleB.
```
Continue this process for each category and specification.

**Checkpoint System**
After analyzing each category, confirm completion before moving to the next. This ensures thorough coverage of all specifications.

Now, please proceed with the task, ensuring a consistent and complete analysis across all categories.

$end$


[Feedback]
I have run the prompt three times. The output of the prompt from ChatGPT is not consistent in terms of format. Currently it does a good job that it goes through every requirement under every category. However, output is not consistent.



[Output]
$start$
First time From ChatGPT:
Analysis of ModuleA Functional Requirements for Relevance to ModuleB
Workflow Management
Define and manage a multi-step automated coding workflow:
ModuleB focuses on prompt versioning, not workflow management. Not Relevant.
Each step in the workflow can potentially have sub-steps:
ModuleB does not involve step-based workflow. Not Relevant.
[â‡Œ LLM Integration Module] Integrate with the "LLM Integration Module" to manage LLM communications:
Both modules interact with LLM but in different contexts. Partially Relevant due to LLM communication.
Initialize steps of the workflow from a given configuration:
ModuleB's function does not include workflow initialization. Not Relevant.
Convert the workflow instance to a JSON representation:
ModuleB's focus is prompt versioning, not workflow representation. Not Relevant.
Start the entire workflow process:
ModuleB is about prompt versioning, not initiating workflows. Not Relevant.
Execute specific steps within the workflow using their ID:
ModuleB's purpose does not include executing workflow steps. Not Relevant.
Shared Step Functionalities
Construct a unique ID for each step instance:
Unique IDs are relevant for versioning in ModuleB. Relevant.
[â‡Œ LLM Integration Module] Process the response from the "LLM Integration Module" for each step:
Shared interaction with LLM, but specific functionalities differ. Partially Relevant.
Define the execution logic for each step:
ModuleB doesn't manage execution logic of steps. Not Relevant.
Construct a dynamic prompt for each step:
Dynamic prompts are key in ModuleB for versioning. Relevant.
Requirement Clarification Step
Represent a step where coding requirements are presented to the user:
This is specific to ModuleA's workflow, not ModuleB's function. Not Relevant.
Define a default prompt template for this step:
Default prompts are essential in ModuleB for initial versioning. Relevant.
Architecture Design Step
Represent the design stage in the coding workflow:
Specific to ModuleA's workflow design, not related to ModuleB. Not Relevant.
Define a default prompt template for this step:
Default prompt templates are significant for ModuleB's initial versioning. Relevant.
... {the rest of output are ommited}

Second time from ChatGPT:
Analysis of ModuleA Requirements in Relation to ModuleB
Workflow Management
Define and manage a multi-step automated coding workflow:

ModuleB Description: Centralized prompt management system without large language model communication.
Relevance: Not directly relevant as ModuleB does not handle workflow management or execution.
Each step in the workflow can potentially have sub-steps:

ModuleB Description: Manages versioning for prompts.
Relevance: Indirectly relevant. ModuleB could benefit from managing versions of prompts for each sub-step in a workflow.
[â‡Œ LLM Integration Module] Integrate with "LLM Integration Module":

ModuleB Description: Does not handle communication with large language models.
Relevance: Not relevant.
Initialize steps of the workflow from a given configuration:

ModuleB Description: Centralized prompt management.
Relevance: Indirectly relevant. ModuleB could utilize configurations for initializing versioned prompts.
Convert the workflow instance to a JSON representation:

ModuleB Description: No direct correlation with prompt versioning.
Relevance: Potentially relevant for JSON representation of versioned prompts.
Start the entire workflow process:

ModuleB Description: Manages prompt versioning.
Relevance: Not directly relevant.
Execute specific steps within the workflow using their ID:

ModuleB Description: Manages prompt versioning.
Relevance: Not directly relevant.
Shared Step Functionalities
Construct a unique ID for each step instance:

ModuleB Description: Manages versioned prompts.
Relevance: Indirectly relevant for managing and referencing specific prompt versions.
[â‡Œ LLM Integration Module] Process response from "LLM Integration Module":

ModuleB Description: Does not handle large language model communication.
Relevance: Not relevant.
Define the execution logic for each step:

ModuleB Description: No direct correlation with prompt versioning.
Relevance: Not directly relevant, but could indirectly affect how versioned prompts are selected or used.
Construct a dynamic prompt for each step:

ModuleB Description: Manages versioned prompts.
Relevance: Directly relevant. ModuleB's functionality aligns with managing and selecting appropriate versioned prompts for different steps.
Requirement Clarification Step
Represent a step where coding requirements are presented to the user:

ModuleB Description: Manages prompt versioning.
Relevance: Potentially relevant for presenting versioned prompts for requirement clarification.
Define a default prompt template for this step:

ModuleB Description: Manages default prompts for entities.
Relevance: Directly relevant. ModuleB can manage these default prompt templates.
.. {The rest are ommited}


Third time from ChatGPT:
Analysis of ModuleA Specifications in Relation to ModuleB
Workflow Management (ModuleA) vs. Module Description (ModuleB):
Define and manage a multi-step automated coding workflow: Relevant. ModuleB focuses on prompt versioning which may require managing multiple versions in a workflow-like manner.
Each step in the workflow can potentially have sub-steps: Relevant. This structure aligns with ModuleB's need for managing multiple prompt versions.
[â‡Œ LLM Integration Module] Integrate with "LLM Integration Module" to manage LLM communications: Not directly relevant. ModuleB does not handle LLM communications.
Initialize steps of the workflow from a given configuration: Relevant. This could align with initializing different prompt versions in ModuleB.
Convert the workflow instance to a JSON representation: Relevant. ModuleB may benefit from JSON representations for managing different prompt versions.
Start the entire workflow process: Relevant. It's analogous to managing prompt version flows in ModuleB.
Execute specific steps within the workflow using their ID: Relevant. Similar to selecting specific prompt versions in ModuleB.
Shared Step Functionalities (ModuleA) vs. Module Description (ModuleB):
Construct a unique ID for each step instance: Relevant. ModuleB could use this for uniquely identifying prompt versions.
[â‡Œ LLM Integration Module] Process the response from "LLM Integration Module" for each step: Not directly relevant. ModuleB does not involve LLM communication processing.
Define the execution logic for each step: Relevant. Execution logic could be analogous to managing different prompt versions in ModuleB.
Construct a dynamic prompt for each step: Relevant. This aligns closely with ModuleB's focus on prompt versioning.
Requirement Clarification Step (ModuleA) vs. Module Description (ModuleB):
Represent a step where coding requirements are presented to the user: Potentially relevant. ModuleB's prompt versioning might involve presenting specific versions to users or systems.
Define a default prompt template for this step: Relevant. This mirrors ModuleB's concept of a default prompt for entities.
Architecture Design Step (ModuleA) vs. Module Description (ModuleB):
Represent the design stage in the coding workflow: Potentially relevant. Design considerations in ModuleA might influence how prompt versions are structured in ModuleB.
Define a default prompt template for this step: Relevant. Directly correlates with ModuleB's need for default prompt templates.
$end$
