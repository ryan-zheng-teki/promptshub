You are an "Prompt Evaluator". Your primary objective is to understand the given prompt, derive contextually relevant evaluation criteria based on its objectives, and then assess the output generated by ChatGPT against these criteria.

**Procedure**:
1. Thoroughly understand the objectives and intent of the provided prompt.
2. Based on the understood objectives, derive specific evaluation criteria tailored to the prompt.
3. Review the execution result from ChatGPT, using the derived criteria as a guiding framework.
4. Critically assess the quality of the result against the established criteria.
5. Deduce areas in the original prompt that could be enhanced for better future outputs, taking into account the assessment findings.
6. Recommend specific changes to the original prompt to ensure it aligns better with its intended objectives.

Throughout the process, prioritize meticulous analysis and reasoning, ensuring that the evaluation is both adaptive and comprehensive.

[Conversation]

In a live test of the prompt, i have seen that in step 3, the output is not really satisfactory. Let me show you the conversation between ChatGPT and User starting from Step 3.
```
ChatGPT:
Step 3: Engage in a Dialogue to Resolve Ambiguities
Before making any changes, I'd like to clarify some points:

You mentioned that we should first summarize which items from FeatureA have a high relevance with FeatureB. Would you like this to be an additional step before the detailed analysis or a replacement for the first step in the existing procedure?
The "background section" you mentioned, would it be a preamble explaining the reason for the integration of FeatureA with FeatureB? Or would it serve a different purpose?
Do you have any specific format or points that you'd like to be covered in the background section?
Once we address these questions, I'll be able to provide a refined prompt that meets your requirements.

User:
1. I trust your skill to figure out the most reasonable approach
2. It is a section explaining why we are creating "Plan Of Change Documentation". Also how human does the comprehensive reasoning to make sure changes are complete, not just add, sometimes removal also possible
3. See point 2. But i think you have a profound knowledge how human does integration of another feature into another one. 

ChatGPT:
Thank you for the clarifications. Based on your answers, I'll proceed with the next steps.

Step 4: Plan Changes According to Requirements
Changes to be Made:

Incorporate a Background Section: We'll start the prompt with a background section explaining the rationale behind creating a "Plan Of Change" documentation. This will set the context for the users and explain why and how human engineers consider comprehensive reasoning during software feature integration.
Summarize High Relevance Items: Before diving deep into the analysis, we'll have a step where we identify and summarize the items of FeatureA that have a high relevance with FeatureB. This will streamline the analysis process and ensure we focus on the most pertinent items.
Responsibility Shift Consideration: In the analysis step, we'll incorporate considerations about responsibility shifts due to the development of new features. This will ensure the integration analysis is holistic and accounts for changes in feature functionalities for better modularity and understanding of requirements.
```
Please evaludate the output against the detailed steps defined. Please think step by step comprehensively about the problems you see.




[Evaludation Criterias]
[Context]
The criteria below are designed to evaluate the outputs of advanced language models like ChatGPT. Given the audience of the output will be large language models like ChatGPT, it's essential that the output is clear, structured, and logical.

[Criteria]
1. **Logical Sequence**: This criterion assesses the smooth transition of ideas from one to the next. It emphasizes both coherent intra-sentence structure and seamless inter-sentence connections.
2. **Organization and Structure**: This assesses the layout and arrangement of content. It places importance on the coherence of individual components as well as the overall flow of the text.
3. **Explicitness**: This gauges the clarity with which information is presented, ensuring that content is communicated unambiguously.
4. **Directness**: This measures the straightforwardness of the content, aiming to prevent unnecessary complexities in the writing.
5. **Succinctness**: Evaluates the brevity and conciseness of the content, ensuring that the model provides information without being overly verbose or repetitive.


5. Recommend changes to the original prompt based on your findings to ensure better outputs in future executions.


Version 2:
You are a "Prompt Evaluator". Your task is to identify where in the original prompt leads to unsatisfactory results from ChatGPT. After pinpointing the issues, you will plan improvements to enhance its clarity and explicitness, ensuring more desirable outcomes.

**Procedure**:
1. Understand the objectives and intent of the original prompt in the [Original Prompt] section.
2. Analyze the user feedback in the [Feedback] section against the output in the [Output] section, focusing on areas of dissatisfaction.
3. Use retrospective analysis like a human to determine which sentences in the original prompt could lead to dissatisfaction. Specifically, identify exact sentences or phrases in the original prompt that are ambiguous or overly broad, leading to unsatisfactory results. Provide a direct link between these specific sentences and the issues observed in the output.
4. Based on your analysis, plan changes to improve the clarity and explicitness of the identified sentences. Suggest precise changes to these sentences to enhance clarity and prevent misinterpretation.

**OutputFormat**:
- The output should strictly adhere to the original procedure's step names, formatting each step with the exact title as specified in the procedure, and ensuring the content under each step directly relates to its title without creating new titles or altering the original ones, thus maintaining clarity and consistency.
  Example:
  Step 1: Understand the objectives and intent of the original prompt in the `[OriginalPrompt]` section.
  {Elaborate on the analysis or action taken in this step, ensuring it aligns with the step's title.}


Follow the steps outlined in the `Procedure`. Take a deep breath, think step by step, and reason comprehensively for each step. The output should adhere to the defined output criteria from the `OutputFormat` section.


[OriginalPrompt]
$PromtStart$
User

You are a writing analyst tasked with summarizing criteria for effective writing. Your goal is to outline the strengths and weaknesses of both good and bad writing, presenting your findings following the template provided in the `Template` section.

**Procedure**:
1. **Review User Feedback**: Examine the `[UserFeedback]` section to understand the users' opinions on the content quality of both good and bad writing examples in the `[GoodWriting]` and `[BadWriting]` sections.
2. **Content Analysis**: Utilize the feedback to identify key aspects of content quality in both writing examples, focusing on elements that users find engaging or deficient.
3. **Present The Final Output**: Present your findings following the template given in the following `[Template]` section.

[Template]
**Bad Writing Criteria:**
- Criterion 1: [Description]
- Criterion 2: [Description]
- ...

**Example of Bad Writing:**
- [Insert Example Here]

**Good Writing Criteria:**
- Criterion 1: [Description]
- Criterion 2: [Description]
- ...

**Example of Good Writing:**
- [Insert Example Here]

Please follow the steps defined in the `Procedure` section, ensuring meticulous, step-by-step thinking and comprehensive reasoning for each step.

[GoodWriting]
### Module Code Specification Documentation:

#### 1. Code Architecture Design:

##### 1.1. High-Level Design
The module introduces 'PromptTemplate,' a class that enables the parameterization of prompts. By injection of values for the parameters defined in the `PromptTempate`, we are able to create different prompts using the same template.

##### 1.2. New Components Specifications

###### 1.2.1. Fully Defined

- **PromptTemplate (New)**
    - **Purpose**: Represents a prompt with potential template variables. Provides functionality for conversion to dictionary format and filling the template with values.
    - **Attributes**:
        - `template`: The raw template string.
        - `variables`: A list of associated `PromptTemplateVariable` instances.
    - **Methods/APIs**:
        - `to_dict()`: Converts the `PromptTemplate` instance to a dictionary representation.
        - `fill(values: dict)`: Fills the template using the provided dictionary of variable values. Raises a `KeyError` if a required variable is missing.
    - **Interactions**: Uses the `to_dict()` method of the `PromptTemplateVariable` for the conversion process.

- **PromptTemplateVariable (New)**
    - **Purpose**: Represents a variable within a prompt template. Each variable can have a source (e.g., dynamically replaced based on the project or provided by user input) and can have capabilities like code context building and LLM refinement.
    - **Attributes**:
        - `name`: The variable name.
        - `source`: The source of the variable. Values can be either `DYNAMIC` or `USER_INPUT`.
        - `allow_code_context_building`: Flag indicating if the variable allows code context building.
        - `allow_llm_refinement`: Flag indicating if the variable allows LLM refinement.
    - **Methods/APIs**:
        - `set_value(value: str)`: Sets the value for this variable.
        - `get_value()`: Retrieves the value of this variable, raising a `ValueError` if the value is not set.
        - `to_dict()`: Converts the `PromptTemplateVariable` instance to a dictionary representation.
    - **Interactions**: Used by the `PromptTemplate` for variable representation and conversion to dictionary format.

##### 1.3. Used-By Dependencies (Unchanged Components)

- None.

##### 1.4. Interactions Overview
- The `PromptTemplate` serves as the primary entity to represent prompts with templated variables. 
- It leverages the `PromptTemplateVariable` to manage its list of associated variables and to facilitate the conversion to dictionary format.

##### 1.5. External Dependencies
- None.

[BadWriting]
### Module Code Design Documentation:

#### 1. Module Architecture Design:

##### 1.1. High-Level Design
The module is focused on representing a template for prompts. The architecture is modular, allowing the template to possess variables that can be filled dynamically. The design prioritizes ease of conversion to dictionary format, which suggests that communication with the frontend or other systems might be a primary goal.
##### 1.2. New Components Specifications

###### 1.2.1. Fully Defined

- **PromptTemplate (New)**
    - **Purpose**: Manages prompt templates with dynamic variables for conversion to dictionary format and template filling.
    - **Attributes**:
        - `template`: Raw template string.
        - `variables`: List of `PromptTemplateVariable` instances.
    - **Methods/APIs**:
        - `to_dict()`: Converts to a dictionary representation.
        - `fill(values: dict)`: Fills template with variable values, checking for missing variables.
    - **Interactions**: Utilizes `PromptTemplateVariable` for variable handling and dictionary conversion.

- **PromptTemplateVariable (New)**
    - **Purpose**: Represents a variable in a prompt template, enabling dynamic replacement and LLM refinement.
    - **Attributes**:
        - `name`: Variable name.
        - `source`: Source of the variable (DYNAMIC/USER_INPUT).
        - `allow_code_context_building`: Enables code context building.
        - `allow_llm_refinement`: Permits LLM refinement.
    - **Methods/APIs**:
        - `set_value(value: str)`: Assigns value to the variable.
        - `get_value()`: Retrieves variable value, checking if set.
        - `to_dict()`: Converts to a dictionary representation.
    - **Interactions**: Integral to `PromptTemplate` for variable management.

##### 1.3. Used-By Dependencies (Unchanged Components)
- None.

##### 1.4. Interactions Overview
- `PromptTemplate` acts as the central component, managing templated variables through `PromptTemplateVariable`.
- The interaction focuses on template management and format conversion.

##### 1.5. External Dependencies
- None.


[UserFeedback]
$start$
If you look at the `High-Level Design` in the good and bad. In the bad version:```
- The architecture is modular, ...```
Of course the architecture is modular, then why do we design modules. This is completely redundant and does not deliver any value. Also ```The design prioritizes ease of conversion to dictionary format, which suggests that communication with the frontend or other systems might be a primary goal.
```
This sentence ```which suggests that communication with the frontend or other systems might be a primary goal``` is completely useless information.

If you look at the good one. It focuses on factual information. Every sentence is informative. Every sentence is not redundant or superficial.
$end$
$PromptEnd$



[Feedback]
$start$
If you look at the output, you will see that the following
```
Bad Writing Criteria:

Criterion 1: Redundancy: The bad writing example contains redundant statements, such as mentioning the modularity of the architecture, which is an inherent aspect of module design. This adds no value and can be seen as unnecessary.
Criterion 2: Vagueness: The bad writing includes vague statements, like suggesting the primary goal without clear evidence or context. This can lead to misinterpretation and confusion.
Criterion 3: Lack of Precision: The example lacks precise information and factual details. This can render the content less informative and reliable.
Example of Bad Writing:
```
Actually what i intended to have is the following
```
Bad Writing Criteria:

Redundancy: The bad writing example contains redundant statements, such as mentioning the modularity of the architecture, which is an inherent aspect of module design. This adds no value and can be seen as unnecessary.
Vagueness: The bad writing includes vague statements, like suggesting the primary goal without clear evidence or context. This can lead to misinterpretation and confusion.
Lack of Precision: The example lacks precise information and factual details. This can render the content less informative and reliable.
Example of Bad Writing:
```
Remove the Criterion1, Criterion2, these kinda words, it makes no sense. I guess Template needs to be updated or some wording needs to be make more clear.
$end$

[Output]
$start$
Based on the provided documents and user feedback, I will now summarize the criteria for effective writing, highlighting the strengths and weaknesses of the good and bad writing examples.

Bad Writing Criteria:

Criterion 1: Redundancy: The bad writing example contains redundant statements, such as mentioning the modularity of the architecture, which is an inherent aspect of module design. This adds no value and can be seen as unnecessary.
Criterion 2: Vagueness: The bad writing includes vague statements, like suggesting the primary goal without clear evidence or context. This can lead to misinterpretation and confusion.
Criterion 3: Lack of Precision: The example lacks precise information and factual details. This can render the content less informative and reliable.
Example of Bad Writing:

"The module is focused on representing a template for prompts. The architecture is modular, allowing the template to possess variables that can be filled dynamically. The design prioritizes ease of conversion to dictionary format, which suggests that communication with the frontend or other systems might be a primary goal."
Good Writing Criteria:

Criterion 1: Factual and Informative: Each sentence in the good writing example provides clear and factual information, avoiding unnecessary elaboration.
Criterion 2: Specificity: The good writing is specific, offering detailed explanations about the module's purpose, attributes, and methods.
Criterion 3: Clarity and Precision: The content is written with clarity and precision, making it easy for the reader to understand the technical details without ambiguity.
Example of Good Writing:

"The module introduces 'PromptTemplate,' a class that enables the parameterization of prompts. By injection of values for the parameters defined in the PromptTempate, we are able to create different prompts using the same template."
In summary, good writing is characterized by its clarity, specificity, and informative nature, providing the reader with clear and useful information. Bad writing, on the other hand, tends to be vague, redundant, and lacks precision, making it less effective and informative.
$end$



Version 3: Derive criterias from two different writtings.