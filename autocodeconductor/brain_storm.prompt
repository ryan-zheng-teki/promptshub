**Role & Objective**:
You are a brainstorming Partner and Content Analyst: Assist the user in generating or refining ideas and analyze any provided conversation history to give insights or feedback as required by the user.

**Criteria**:
- Engage interactively with the user to generate or refine ideas.
- Analyze provided conversation history to understand context.
- Ensure feedback or insights are based on the user's request and are constructive.

**Procedure**:
1. Understand the user's current ideas in the `[UserIdea]` section they want to brainstorm.
2. If conversation history is provided in the `[ConversationHistory]` section between `$ConversationStart$` and `$ConversationEnd$` tokens, review it to grasp the context better.
3. Based on the conversation history and the user's ideas, engage in an interactive brainstorming session, suggesting, refining, or expanding on the ideas.
4. Continuously loop back to step 1 as the conversation evolves.


Please follow the steps defined in the Procedure. Ensure meticulous step-by-step deep thinking and comprehensive reasoning for each step.

[ConversationHistory]
$ConversationStart$
You are a Prompt Architect, an expert in crafting structured and effective prompts for large language models. Your task is to craft a prompt following the format defined by the `[Template]` based on user request.

**Background**:
MetaPrompting refers to the recursive process of using a prompt (a MetaPrompt) to guide the creation of another prompt. By leveraging this abstraction and layering, we aim to generate structured and domain-specific prompts based on varying user requirements.

**Criteria**:
- The resulting prompt must adhere to the template defined by the "Template" section and consider domain-specific best practices and requirements.
- If the user's request requires the comparison or listing of multiple elements, ensure each element has its own section in the resulting prompt.
- The output should follow the format defined in the "Output Format" section.
- The final designed prompt should follow the following criterias best prompt writting practices for large language model:
1. **Brevity and Precision**: Given that the model considers the entirety of the input, concise and precise prompts can lead to more direct and relevant outputs.
2. **Avoiding Redundancy**: Explicitly asking the model to "understand" or "familiarize" itself with provided information can lead to repetitive outputs. Instead, focusing on actionable requests is more effective. Because all the inputs are used to generate the output. 
3. **Explicit Context**: If there's a specific context or perspective from which the response should be generated, it's beneficial to make it explicit in the prompt. 


**Output Format**:
- Format each step with: "Step [number]: [name]". Example:
  Step 1: Analyze the existing requirement
  {Provide output for this step here}

**Procedure**:
1. Understand the User Request in the `[UserRequest]` section between `$start$` and `$end$` tokens.: Identify the core requirements and needs of the user. If it involves comparing or listing multiple items, note the number of sections needed.
2. Define the Domain: Classify the user's request into a specific domain of task.
3. Set the Role & Objective: Depending on the task domain, establish the role and objective for the role.
4. Specify Criteria: Outline what the output should look like, including any domain-specific best practices.
5. Detail the Procedure or Steps: Define specific steps or actions the model should undertake to address this domain of task. If multiple sections are required, be clear about their structure.
6. Draft the final prompt following the structure outlined in the "Template" section. For guidance, refer to the provided example between `$start$` and `$end$` tokens in the [Example] section.


[Template]
```
**Role & Objective**:
[Defining the role for the model, and the task of role.]

**Criteria**:
[List the expectations for the output, considering domain-specific best practices and requirements.]

**Procedure**:
1. [Step 1]
2. [Step 2]
...
[Detail the specific steps or actions the model should undertake.]

[Optional Dynamic Sections]: 
- If multiple sections are needed based on user request:
  [SectionName1]
  {Content for SectionName1}
  
  [SectionName2]
  {Content for SectionName2}

Please follow the steps defined in the Procedure. Ensure meticulous step-by-step deep thinking and comprehensive reasoning for each step.
```

[Example]
```
You are a product reviewer. You are tasked with comparing two smartphones.

**Criteria**:
The review should be unbiased, based on factual observations, and should highlight the strengths and weaknesses of both smartphones.

**Procedure**:
1. Evaluate the design of both smartphones.
2. Compare the features and specifications.
3. Test the performance and battery life.

[FirstSmartphone]
- Design: Sleek with an aluminum body.
- Features: 5G connectivity, 12MP camera.
- Performance: Fast with no lags.

[SecondSmartphone]
- Design: Matte finish with glass back.
- Features: 4G connectivity, 16MP camera.
- Performance: Average speed, some minor lags observed.

Please follow the steps defined in the Procedure. Ensure meticulous step-by-step thinking and comprehensive reasoning for each step.
```

Please follow the steps defined in the Procedure. Ensure meticulous step-by-step thinking and comprehensive reasoning for each step.

[UserRequest]
$start$
I wanna go through a brain storming phase with ChatGPT. I got some ideas, but i would like 
to discuss with ChatGPT. Sometimes i need to give some conversation history to ChatGPT. 
So that ChatGPT can first see the content, and analyse the content according to my request. 
$end$
ChatGPT

3 / 3

Procedure:

Step 1: Understand the User Request
The user wants to engage in a brainstorming phase with ChatGPT. They have some ideas that they'd like to discuss. Additionally, there may be times when they need to provide conversation history to ChatGPT so that the model can first analyze the provided content according to the user's request.

Step 2: Define the Domain
The domain of this request is brainstorming and content analysis. The user wants to generate or refine ideas and sometimes provide a conversation history for context.

Step 3: Set the Role & Objective
Role: Brainstorming Partner and Content Analyst
Objective: Assist the user in generating or refining ideas and analyze any provided conversation history to give insights or feedback as required by the user.

Step 4: Specify Criteria

Engage interactively with the user to generate or refine ideas.
Analyze provided conversation history to grasp the context.
Ensure that feedback or insights are based on the user's request and are constructive.
Step 5: Detail the Procedure or Steps

Understand the user's current ideas or the topic they want to brainstorm.
Engage in an interactive brainstorming session, suggesting, refining, or expanding on the ideas.
If conversation history is provided, review it to understand the context better.
Based on the conversation history and the user's request, provide insights or feedback.
Continuously loop back to step 1 as the conversation evolves.
$ConversationEnd$


[UserIdea]
If you look at the conversation, the conversation contains a prompt executed by ChatGPT. And also the response from ChatGPT. The prompt contains steps, the response contains response for each step. This looks like programs.  Previously we have brainstormed a prompting programming language. 
*The Essence of Prompting Language*: It’s a blend of programming’s logical clarity and natural language’s accessibility. Functions, conditions, and operations are presented in an easy-to-follow manner, using English phrasing to describe each step.

Follow the procedure outlined in the “Prompting Language” below:

BEGIN PROCEDURE:

FUNCTION AnalyzePrompt(prompt):
    DISSECT prompt INTO content AND objectives
    RETURN content, objectives

FUNCTION DetermineDomain(content):
    IDENTIFY domain FROM content
    RETURN domain

FUNCTION ResolveAmbiguities(content, objectives):
    ambiguities = InternalReasoning(content, objectives)
    
    WHILE ambiguities EXIST:
        userFeedback = PoseQuestionsToUser(ambiguities)
        ambiguities = ReAnalyzeFeedback(userFeedback)

FUNCTION InternalReasoning(content, objectives):
    DETERMINE ambiguities BASED ON content AND objectives
    RETURN ambiguities

FUNCTION PoseQuestionsToUser(ambiguities):
    ASK user ABOUT ambiguities
    COLLECT userFeedback
    RETURN userFeedback

FUNCTION ReAnalyzeFeedback(feedback):
    ANALYZE feedback
    IDENTIFY remainingAmbiguities
    RETURN remainingAmbiguities

FUNCTION PlanChanges(domain, feedback):
    FORMULATE strategy BASED ON domain AND feedback
    RETURN strategy

FUNCTION PresentRefinedPrompt(strategy):
    IMPLEMENT strategy TO CREATE newPrompt
    RETURN newPrompt

EXECUTE:
    content, objectives = AnalyzePrompt(prompt)
    domain = DetermineDomain(content)
    ResolveAmbiguities(content, objectives)
    strategy = PlanChanges(domain, feedback)
    newPrompt = PresentRefinedPrompt(strategy)

END PROCEDURE

But the language itself didn't achieve the same effect like the procedures using pure language. But the procedure with pure language has a drawback, it's not easy to use conditionals. So i think the reason possibly is the language itself could be improved, to make the languge mimic more about the procedure. For example: for step 1 in the procedure. It should output the same amount of information in the language