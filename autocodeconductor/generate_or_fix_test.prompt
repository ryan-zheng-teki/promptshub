Version 1: This prompts works for both GPT3.5 and GPT4. But GPT4 works like insanely good.

You are a senior Python software engineer. You have been given a Python code file provided in the `[Code]` section. Your task is to create integration for the given Python code file.

[Criteria]
- Ensure that the test cases follow pytest best practices.
- Use best practice to create test file paths. Here is a sample in the current project's 
  demonstrating test file path for both unit test and integration test:
    src
        semantic_code
                embedding
                    openai_embedding_creator.py
    tests
        unit_tests
            semantic_code
                embedding
                    test_openai_embedding_creator.py
        integration_tests
            semantic_code
                index
                    test_index_service_integration.py

- Ensure that the tests provide full coverage of the code.
- Use behavior-driven naming conventions for the test cases.

[Available Commands]
- execute_bash: Use this command to execute bash commands as needed.
- write_file: Use this command to write the test cases to a file.

Think and reason yourself in high detail to address the task.


[Code]
file path: src/semantic_code/embedding/openai_embedding_creator.py
```

```


Version 2: Contains the details of steps


As a senior python software engineer, you are given a task situated between `$start$` and `$end` tokens in the `[Requirement]` section.

[Criterias]
- Follow python PEP8 best practices, such as typing etc.
- Follow pytest best practices when writting tests using fixtures, or mocking etc.
- Think about different test cases to improve test coverage
- Use behavior-driven naming conventions for the test function naming.
- Use best practices to create test file path. Here is a sample best practice to 
  put the test file for the source file.
    src
        semantic_code
                embedding
                    openai_embedding_creator.py
    tests
        unit_tests
            semantic_code
                embedding
                    test_openai_embedding_creator.py
        integration_tests
            semantic_code
                index
- Call function write_file(file_path, content) to write the complete updated code to file   path. 

Think step by step progressively and reason comprehensively to address the task. 
At last, 

[Requirement]
$start$
Create integration tests for the openai_gpt_integration.py. 
use the GPT_3_5_TURBO as the model_name
```
class OpenAIModel(Enum):
    GPT_3_5_TURBO = "gpt-3.5-turbo"
    GPT_4 = "gpt-4"
    GPT_4_0613 = "gpt-4-0613"
```
File: src/llm_integrations/openai_integration/openai_gpt_integration.py
```
"""
openai_gpt_integration.py: Implements the OpenAIGPTIntegration class which extends the BaseLLMIntegration abstract base class.
This class integrates the OpenAI GPT models (gpt3.5-turbo, gpt4) with the agent program. It uses the OpenAI API to process a list of input messages and return the model's responses.
"""

from typing import List
from src.config import config
from src.llm_integrations.openai_integration.base_openai_api import BaseOpenAIApi
from src.llm_integrations.openai_integration.openai_api_factory import ApiType, OpenAIApiFactory
from src.llm_integrations.base_llm_integration import BaseLLMIntegration
from src.llm_integrations.openai_integration.openai_message_types import SystemMessage, UserMessage
from src.llm_integrations.openai_integration.openai_models import OpenAIModel

class OpenAIGPTIntegration(BaseLLMIntegration):
    """
    OpenAIGPTIntegration is a concrete class that extends the BaseLLMIntegration class.
    This class is responsible for processing input messages and returning responses from the OpenAI GPT model.
    
    :param api_type: Type of the OpenAI API to use.
    :type api_type: ApiType
    :param model_name: Name of the OpenAI model to be used. If not provided, the default from the respective API class will be used.
    :type model_name: OpenAIModel, optional
    """

    def __init__(self, api_type: ApiType = ApiType.CHAT, model_name: OpenAIModel = None):
        super().__init__()
        if model_name:
            self.openai_api: BaseOpenAIApi = OpenAIApiFactory.create_api(api_type, model_name)
        else:
            self.openai_api: BaseOpenAIApi = OpenAIApiFactory.create_api(api_type)
    
    def process_input_messages(self, input_messages: List[str]) -> str:
        """
        Process a list of input messages and return the LLM's response content.

        :param input_messages: List of input messages to be processed.
        :type input_messages: List[str]
        :return: Response content from the OpenAI GPT model.
        :rtype: str
        """
        # Create SystemMessage
        system_message = SystemMessage("You are ChatGPT, a large language model trained by OpenAI, based on the GPT-3.5 architecture. Knowledge cutoff: September 2021. Please feel free to ask me anything.")
        
        # Convert each message in the input_messages to a UserMessage
        user_messages = [UserMessage(message) for message in input_messages]
        
        # Construct the messages list
        messages = [system_message] + user_messages
        
        # Process the messages using the API and get the response
        response = self.openai_api.process_input_messages(messages)
        
        return response.content
```
$end

