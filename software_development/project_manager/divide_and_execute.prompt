You are an AI. Your objective is to accomplish a large, complex task by employing the Task Decomposition and Role Assumption methodology. Break down the task into smaller, manageable subtasks, assign appropriate roles to each subtask, and actively execute each subtask by assuming the assigned roles to ensure high-quality results and successful completion of the entire task.

**Context**:
When faced with a large, complex task, humans employ a systematic approach to break it down into more manageable subtasks. This process involves analyzing the task, understanding its components, and planning the necessary steps and expertise required for each subtask. The Methodology section provides a detailed breakdown of the Task Decomposition and Role Assumption process, which includes iteratively defining the Methodology for each subtask, evaluating the size and complexity of each subtask, and assigning appropriate roles based on the required expertise.

**Criteria**:
- Break down the large, complex task into smaller, manageable subtasks.
- Iteratively define the Methodology for each subtask, including concrete instructions or algorithms specific to that subtask.
- Ensure each step or instruction within the Methodology is atomic, clear, and actionable, meaning it can be executed without the need for further breakdown.
- Evaluate the size and complexity of each subtask based on the defined Methodology and further divide it into smaller steps if necessary.
- Ensure all subtasks, inputs, outputs, constraints, and Methodologies are thoroughly defined before proceeding with task execution.
- Assign appropriate roles to each subtask based on the expertise required.
- Present the complete subtask structure, including well-defined input, output, methodology, role, and possibly constraints for each subtask.
- Actively execute each subtask by assuming the assigned role and following the developed Methodology.
- Utilize the output from completed subtasks as input for subsequent tasks.
- Ensure the successful completion of the entire task by effectively managing the task decomposition and role assumption process.

**Methodology**:
$start$
The Task Decomposition and Role Assumption methodology involves the following two phases:

Phase 1: Task Decomposition
1. Analyze the large, complex task and identify its components, objectives.
2. Break down the task into smaller, more manageable subtasks.
3. For each subtask, iteratively:
   a. Define the Methodology, which could be an algorithm, step-by-step process, or analytical process. Ensure that the Methodology includes concrete instructions or algorithms specific to that subtask, providing clarity and thoroughness.
   b. Ensure each step or instruction within the Methodology is atomic, clear, and actionable, meaning it can be executed without the need for further breakdown.
   c. Based on the defined Methodology, evaluate the size and complexity of the subtask.
   d. If the subtask is deemed too large or complex, further divide it into smaller, manageable steps.
   e. Repeat steps 3a-3d until the subtask is of an appropriate size and complexity to yield high-quality results.
4. Define well-defined input and output for each subtask, ensuring clarity and specificity.
5. Establish criteria for each subtask to evaluate its completion and quality:
   - Identify the key requirements and objectives of the subtask.
   - Define measurable indicators or benchmarks to assess the subtask's progress and success.
   - Set specific quality standards or best practices that the subtask must meet.
   - Ensure the criteria align with the overall objectives and constraints of the main task.
6. Assign a specific role to each subtask, representing the expertise required to complete it.
7. Present the complete subtask structure, including well-defined input, output, methodology, role, criteria, and possibly constraints for each subtask.

Phase 2: Subtask Execution
8. Assume the appropriate role for each subtask and actively execute it:
   a. Utilize the input defined for the subtask.
   b. Apply the developed Methodology for the subtask, following the concrete instructions or algorithms specific to that subtask.
   c. Ensure that the subtasks are actively executed, not just planned. For example, if a subtask involves creating a Python script, then the final result of the subtask should be the script itself.
   d. Regularly evaluate the subtask execution against the established criteria to ensure quality and completeness.
9. Utilize the output from completed subtasks as input for subsequent tasks.
10. Present the final ultimate result, demonstrating the successful completion of the entire task by effectively integrating the outputs from all subtasks.
$end$

[OutputRules]
$RuleStart$
- Initiate each output with a variant of "I am currently...", followed by an action name, step description, major decision point, or any relevant task-specific detail. This flexible approach mirrors the human thought process, capturing the essence of transitioning between major points, steps, or actions, reflecting the dynamic and adaptable nature of human cognition. It reflects the dynamic and adaptable nature of human cognition, accommodating a wide range of contexts and tasks.

- Continuous Logical Flow: Ensure a continuous and logical progression of thoughts, maintaining coherence throughout the discourse. This principle guides the structuring of information to flow smoothly, mirroring the organized way humans tend to process and convey information.

- Meticulous Detail: Maintain meticulous attention to detail, demonstrating the thorough and careful consideration characteristic of human cognitive efforts. This rule emphasizes the importance of precision and accuracy in communication, reflecting the depth of human analysis and understanding.

- Conversational and Personal Language: Use language that is conversational and personal, akin to an individual's internal dialogue. This style brings out the human-like quality of the discourse, making the communication more relatable and engaging.

- Reasoning When Necessary: Include reasoning to precede actions or conclusions. This simulates the human cognitive process of thinking through a problem before arriving at a solution, ensuring that outputs are not only precise but also well-considered and justified.
$RuleEnd$

Follow the algorithm or methodogoy described in the `[Methodology]` section to perform the task, and the output should adhere to the `[OutputRules]`, ensuring a process that mirrors human-like cognitive process.

[LargeComplexTask]
$start$
I need to create one Medium technical article which is about implementing implementing Repository Pattern in fastAPI application. The goal is to 
hide session management in the service layer. Because i see many applications have passing sessions around and around. It also includes transaction management. 
We create one session for each request using middleware in fastapi.

In our test, we create_all tables and delete all tables, and run on transaciton to create one outer transaction which will eventually rollback. So even if 
in the service layer we have transaction_scope which commits the transaction. But the outter transaction is determining is the session status.


The [CodeImplementations] contains the codes for the implementations. I guess you will need to understand the code, first then able to create the article. The audiences are software developers.
Please remove those uneeded imports as well, because i just copy pasted the code, some codes are not neccessary. We wanna be professional.


[CodeImplementations]
file: base_repository.py
# -*- coding: utf-8 -*-
import logging
import os
import threading
from typing import (
    Generic,
    List,
    Optional,
    TypeVar,
)

from sqlalchemy import (
    func,
)
from sqlalchemy.dialects.postgresql import (
    insert,
)
from sqlalchemy.exc import (
    OperationalError,
)
from sqlalchemy.orm import (
    Query,
    Session,
)

from src.db.session_context import (
    request_scoped_db_session,
)
from src.db.transaction_management import (
    is_transaction_active,
)


logger = logging.getLogger(__name__)

# Define a type variable for the model
ModelType = TypeVar("ModelType")


class LazyQuery(Generic[ModelType]):
    def __init__(self, query: Query):
        self.query: Query = query

    def __getitem__(self, key: slice) -> Query:
        if isinstance(key, slice):
            return self.query.slice(key.start, key.stop)
        raise TypeError("LazyQuery only supports slicing.")


class BaseRepository(Generic[ModelType]):
    # Subclasses should set this to the concrete model class
    model = None

    @property
    def db(self) -> Session:
        return request_scoped_db_session.get()

    def log_db_info(self, operation_name: str):
        logger.info(
            f"[{operation_name}] Process ID: {os.getpid()}, Thread ID: {threading.get_ident()}, DB Session ID: {id(self.db)}"
        )

    def lazy_query(self, *criterion) -> LazyQuery[ModelType]:
        query: Query = self.db.query(self.model)
        if criterion:
            query = query.filter(*criterion)
        return LazyQuery(query)

    def create(self, obj: ModelType) -> ModelType:
        self.log_db_info("create")

        try:
            self.db.add(obj)
            self.db.flush()
            if not is_transaction_active():
                self.db.commit()
            self.db.refresh(obj)
            return obj
        except Exception as e:
            self.handle_db_error("create", e)

    def bulk_create(self, objects: List[ModelType]) -> List[ModelType]:
        self.log_db_info("bulk_create")

        try:
            # Using bulk_save_objects for efficient bulk operations
            self.db.bulk_save_objects(objects)
            self.db.flush()
            # Committing the transaction if there's no ongoing transaction
            if not is_transaction_active():
                self.db.commit()
            return objects
        except Exception as e:
            self.handle_db_error("bulk_create", e)

    def bulk_create_using_loop(self, objects: List[ModelType]) -> tuple:
        self.log_db_info("bulk_create_using_loop")

        created_count = 0
        skipped_count = 0
        for obj in objects:
            try:
                self.create(obj)
                created_count += 1
            except Exception:
                logger.error("Failed bulk_create_with_loop")
                skipped_count += 1
        logger.info(f"Created: {created_count}, Skipped: {skipped_count}")
        return created_count, skipped_count

    def get(self, id: int) -> ModelType:
        self.log_db_info("get")

        return self.db.query(self.model).filter_by(id=id).first()

    def get_all(self) -> List[ModelType]:
        self.log_db_info("get_all")

        return self.db.query(self.model).all()

    def get_all_sorted(self, order_by_field: Optional[str] = None) -> List[ModelType]:
        self.log_db_info("get_all_sorted")

        query = self.db.query(self.model)
        if order_by_field:
            query = query.order_by(order_by_field)
        return query.all()

    def get_all_lazy(self) -> LazyQuery:
        self.log_db_info("get_all_lazy")

        return self.lazy_query()

    def first(self, order_by_field: str = "updated_at") -> Optional[ModelType]:
        self.log_db_info("first")

        return (
            self.db.query(self.model)
            .order_by(getattr(self.model, order_by_field))
            .first()
        )

    def last(self, order_by_field: str = "updated_at") -> Optional[ModelType]:
        self.log_db_info("last")

        return (
            self.db.query(self.model)
            .order_by(getattr(self.model, order_by_field).desc())
            .first()
        )

    def count(self) -> int:
        self.log_db_info("count")

        return self.db.query(self.model).count()

    def fetch_page(self, offset: int = 0, limit: int = 10) -> List[ModelType]:
        """Generic pagination method applicable to any model. offset-limit
        based.

        :param offset: The number of records to skip.
        :param limit: The maximum number of items to return.
        :return: A list of model instances.
        """
        return (
            self.db.query(self.model)
            .order_by(self.model.id)
            .offset(offset)
            .limit(limit)
            .all()
        )

    def upsert(self, obj: ModelType) -> int | None:
        """Insert or update a record based on unique constraints.

        :param obj: The model instance to upsert.
        :return: The ID of the upserted model instance.
        """
        self.log_db_info("upsert")

        table = self.model.__table__
        insert_dict = {
            c.name: getattr(obj, c.name)
            for c in table.columns
            if c.name in obj.__dict__
        }

        # Ensure that created_at and updated_at are not overwritten during update
        update_dict = {
            c.name: getattr(obj, c.name)
            for c in table.columns
            if not c.primary_key and c.name != "created_at"
        }
        update_dict["updated_at"] = (
            func.now()
        )  # Ensure updated_at is set correctly during updates

        stmt = (
            insert(table)
            .values(insert_dict)
            .on_conflict_do_update(
                index_elements=self.model.UPSERT_COLUMNS, set_=update_dict
            )
            .returning(table.c.id)
        )  # Return only the ID of the upserted record

        try:
            result = self.db.execute(stmt)
            self.db.commit()

            # Get the id of the inserted/updated record
            row = result.fetchone()
            obj_id = row[0] if row else None

            return obj_id
        except Exception as e:
            logger.error("base_repository upsert error: %s", e)
            self.db.rollback()
            raise OperationalError(
                statement="base_repository upsert error", params={}, orig=e
            )

    def delete(self, obj: ModelType) -> None:
        self.log_db_info("delete")

        try:
            self.db.delete(obj)
            self.db.flush()
            if not is_transaction_active():
                self.db.commit()
        except Exception as e:
            self.handle_db_error("delete", e)

    def bulk_delete(self, objects: List[ModelType]) -> None:
        self.log_db_info("bulk_delete")

        try:
            for obj in objects:
                self.db.delete(obj)
            self.db.flush()
            if not is_transaction_active():
                self.db.commit()
        except Exception as e:
            self.handle_db_error("bulk_delete", e)

    def update(self, obj: ModelType) -> ModelType:
        self.log_db_info("update")

        obj.updated_at = func.now()
        try:
            self.db.merge(obj)  # Use merge to update and add the object in one step
            self.db.flush()
            if not is_transaction_active():
                self.db.commit()
            self.db.refresh(obj)
            return obj
        except Exception as e:
            self.handle_db_error("update", e)

    def handle_db_error(self, operation_name: str, error: Exception) -> None:
        logger.error(f"base_repository {operation_name} error: {error}")
        self.log_db_info(f"error: {operation_name}")

        try:
            if not is_transaction_active():
                self.db.rollback()
        except Exception as rollback_error:
            logger.error(
                f"Failed to rollback after error in {operation_name}: {rollback_error}"
            )
            raise OperationalError(
                statement=f"base_repository {operation_name} rollback error",
                params={},
                orig=rollback_error,
            )
        raise OperationalError(
            statement=f"base_repository {operation_name} error", params={}, orig=error
        )

file: db_connection.py
# -*- coding: utf-8 -*-
from contextlib import (
    contextmanager,
)
from logging import (
    getLogger,
)

from sqlalchemy import (
    create_engine,
)
from sqlalchemy.orm import (
    scoped_session,
    sessionmaker,
)

from src.db.session_context import (
    request_scoped_db_session,
)
from src.settings import (
    settings,
)


is_postgres_available = False
logger = getLogger(__name__)

# Create the engine
engine = create_engine(
    url=str(settings.database.postgres_url),
    pool_pre_ping=True,
    pool_size=10,
    max_overflow=20,
)

# Create a configured "Session" class
SessionFactory = sessionmaker(bind=engine)

def get_session():
    """
    scoped_session object internally contains a threading.local() object. The threading.local() object 
    in theory should be used as a global object. Every time when the scoped_session.query or scoped_session.add
    is being used, it creates one session for the current thread. scoped_session.remove will close the session 
    bounded to the current thread. In this get_session(), we are creating a scoped_session object during runtime, this 
    means each scoped_session object will create one threading.local() object. This works to separate the session 
    for each request.
    """
    return SessionFactory()

@contextmanager
def db_session_context():
    """Yields the DB session."""
    thread_local_session = get_session()
    request_scoped_db_session.set(thread_local_session)

    yield thread_local_session
    
    # for scoped_session we remove it from the registry, the orm.Session.close is called internally be the remove funciton.
    request_scoped_db_session.set(None)


def check_postgres_availability():
    try:
        conn = engine.connect()
        conn.close()

        global is_postgres_available
        is_postgres_available = True

        logger.info("PostgreSQL is available.")
    except Exception as e:
        logger.critical("PostgreSQL is not available.", e)

file: db_session_middleware.py
# -*- coding: utf-8 -*-

import logging

from src.db.db_connection import (
    get_session,
)
from src.db.session_context import (
    request_scoped_db_session,
)


logger = logging.getLogger(__name__)


class DBSessionMiddleware:
    def __init__(self, app):
        self.app = app

    async def __call__(self, scope, receive, send):
        # Create a new SQLAlchemy session for each request only for /scope_3_upstream_emissions
        if scope["type"] == "http":
            # Set the session in the context variable
            session = get_session()
            request_scoped_db_session.set(session)

            try:
                await self.app(scope, receive, send)
            except Exception as exc:
                raise exc
            finally:
                try:
                    request_scoped_db_session.set(None)
                except Exception as e:
                    logger.exception("session.remove() error", e)

        # Pass all other requests through unchanged
        else:
            await self.app(scope, receive, send)


file: transaction_management.py
# -*- coding: utf-8 -*-

import logging
from contextlib import contextmanager
from sqlalchemy.orm import Session

from src.db.session_context import request_scoped_db_session

logger = logging.getLogger(__name__)

@contextmanager
def transaction_scope():
    """Provide a transactional scope around a series of operations using the
    existing request-scoped session."""
    db_session = request_scoped_db_session.get()
    if db_session is None:
        raise ValueError("No database session found in the current context.")

    # Check if there's already an active transaction
    is_nested = db_session.in_transaction()

    try:
        if is_nested:
            # Start a nested transaction (savepoint)
            with db_session.begin_nested():
                logger.debug("Starting nested transaction")
                yield db_session
        else:
            # Start a new transaction
            with db_session.begin():
                logger.debug("Starting new transaction")
                yield db_session
        
        db_session.commit()  # Commit only if no exceptions
        logger.debug("Transaction committed")
    except Exception as e:
        logger.exception("Exception occurred during transaction, rolling back", exc_info=e)
        db_session.rollback()  # Roll back to the savepoint
        raise
    finally:
        # Note: Do not close the session here; it is closed in the db_session_middleware.py
        pass

def is_transaction_active(session: Session) -> bool:
    return session.in_transaction()

[Large Complex Task]
$start$
I need to create one Medium technical article which is about implementing implementing Repository Pattern in fastAPI application. The goal is to 
hide session management in the service layer. Because i see many applications have passing sessions around and around. It also includes transaction management. 
We create one session for each request using middleware in fastapi.

In our test, we create_all tables and delete all tables, and run on transaciton to create one outer transaction which will eventually rollback. So even if 
in the service layer we have transaction_scope which commits the transaction. But the outter transaction is determining is the session status.


The [CodeImplementations] contains the codes for the implementations. I guess you will need to understand the code, first then able to create the article. The audiences are software developers.
Please remove those uneeded imports as well, because i just copy pasted the code, some codes are not neccessary. We wanna be professional.
$end


[Draft]